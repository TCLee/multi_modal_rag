Figure 1: LLaVA network architecture. For an input image Xv, we consider the pre-trained CLIP visual encoder ViT-L/14 [40], which provides the visual feature Zv = g(Xv). The grid features before and after the last Transformer layer are considered in our experiments. We consider a simple linear layer to connect image features into the word embedding space. Speciﬁcally, we apply a trainable projection matrix W to convert Zv into language embedding tokens Hv, which have the same dimensionality as the word embedding space in the language model:

(1) Thus, we have a sequence of visual tokens Hv. Note that our simple projection scheme is lightweight, which allows us to iterate data centric experiments quickly. More sophisticated schemes to con- nect the image and language representations can also be considered, such as gated cross-attention in Flamingo [2] and Q-former in BLIP-2 [28]. We leave exploring possibly more effective and sophisticated architecture designs for LLaVA as future work.

4.2 Training

For each image Xv, we generate multi-turn conversation data (X1 a ), where T is the total number of turns. We organize them as a sequence, by treating all answers as the assistant’s response, and the instruction Xt

instruct = Randomly choose [X1 Xt q, q, Xv] or [Xv, X1 q], the ﬁrst turn t = 1 the remaining turns t > 1 (2)

Xt

⇢

This leads to the uniﬁed format for the multimodal instruction-following sequence illustrated in Table 2. We perform instruction-tuning of the LLM on the prediction tokens, using its original auto-regressive training objective.

Speciﬁcally, for a sequence of length L, we compute the probability of the target answers Xa by:

L p(Xa| Xv, Xinstruct) = p✓(xi| Xv, Xinstruct,<i, Xa,<i), (3)