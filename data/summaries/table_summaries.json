[
    "This table shows the performance of a model on a conversation task, with different levels of detail and complex reasoning. The model was trained with and without instruction tuning. The results show that instruction tuning improves performance across all metrics, with the largest gains in detail and complex reasoning. The model performs best with full data, followed by detail + complex, and then conversation. The model performs worst with no instruction tuning. \n",
    "This table compares the performance of four different language models (OpenFlamingo, BLIP-2, LLaVA, and LLaVA†) on four tasks: Conversation Detail description, Complex reasoning, All, and OpenFlamingo. The models are evaluated based on their accuracy scores, with higher scores indicating better performance. LLaVA and LLaVA† consistently outperform the other models across all tasks. \n",
    "This table compares the performance of different methods for a task involving natural language understanding (NAT), subject matter (SOC), local area network (LAN), and context. It includes both representative and state-of-the-art (SoTA) methods, as well as results from the authors' own experiments. The table shows the accuracy scores for each method across different grade levels (G1-6, G7-12, and average), as well as the modality used (IMG for image, TXT for text, and NO for none). The methods include GPT-3.5, GPT-3.5 with Chain-of-Thought (CoT), LLaMA-Adapter, MM-CoTBase, MM-CoTLarge, GPT-4, LLaVA, and LLaVA combined with GPT-4 in different configurations. \n",
    "This table shows the performance of different training methods for a 7B model on a visual feature task. The \"Before\" column shows the baseline performance, while the other columns show the performance after different training methods: \"Last Best variant\", \"Predict answer first\", \"Training from scratch\". The results indicate that \"Last Best variant\" achieves the highest performance, followed by \"Predict answer first\" and \"Training from scratch\". \n"
]