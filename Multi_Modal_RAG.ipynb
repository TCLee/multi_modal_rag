{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d63639f-7af0-4971-9542-2cd76910dd89",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before we begin, please make sure you have setup the `.env` file in the project \n",
    "directory as described in [`README.md`](README.md).\n",
    "\n",
    "Next, we will load in the necessary environment variables (e.g., API keys) for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ca59d65-2e5a-496a-9992-4fe09ecb6922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()\n",
    "\n",
    "assert os.environ.get(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afbb58b-ad5d-4d41-b739-cd4bc5e47bce",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "### Partioning and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d9a2e4c-89d3-4b99-a930-2a7f78c78c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.ingest.v2.pipeline.pipeline import Pipeline\n",
    "from unstructured.ingest.v2.interfaces import ProcessorConfig\n",
    "from unstructured.ingest.v2.processes.connectors.local import (\n",
    "    LocalIndexerConfig,\n",
    "    LocalDownloaderConfig,\n",
    "    LocalConnectionConfig,\n",
    "    LocalUploaderConfig\n",
    ")\n",
    "from unstructured.ingest.v2.processes.partitioner import PartitionerConfig\n",
    "from unstructured.ingest.v2.processes.chunker import ChunkerConfig\n",
    "\n",
    "\n",
    "def create_data_ingestion_pipeline(\n",
    "    input_pdf_path: str,\n",
    "    output_dir: str,\n",
    "    image_output_dir: str,\n",
    ") -> Pipeline:\n",
    "    \"\"\"\n",
    "    Creates a data ingestion pipeline. \n",
    "    \n",
    "    The data ingestion pipeline runs the \n",
    "    following tasks:\n",
    "    1. Loads in a PDF document.\n",
    "    2. Partitions the PDF using Unstructured's `hi_res` \n",
    "       strategy into elements (text, tables, images).\n",
    "    3. Chunks the PDF by title.\n",
    "    4. Writes the partitioned and chunked elements \n",
    "       into a JSON file.\n",
    "\n",
    "    Args:\n",
    "        input_pdf_path: Path to the PDF document.\n",
    "        output_dir: Path to directory to write the results to.\n",
    "        image_output_dir: Path to directory to save the \n",
    "            extracted images to.\n",
    "\n",
    "    Returns:\n",
    "        A data ingestion pipeline that can be run.\n",
    "    \"\"\"\n",
    "    return Pipeline.from_configs(\n",
    "        context=ProcessorConfig(\n",
    "            verbose=True,\n",
    "            tqdm=True,\n",
    "        ),\n",
    "        indexer_config=LocalIndexerConfig(\n",
    "            input_path=input_pdf_path,\n",
    "        ),\n",
    "        downloader_config=LocalDownloaderConfig(),\n",
    "        source_connection_config=LocalConnectionConfig(),\n",
    "        partitioner_config=PartitionerConfig(\n",
    "            partition_by_api=False,\n",
    "            strategy=\"hi_res\",        \n",
    "            additional_partition_args={\n",
    "                \"languages\": [\"eng\"],            \n",
    "                \"extract_images_in_pdf\": True,\n",
    "                \"extract_image_block_types\": [\"Image\", \"Table\"],\n",
    "                \"extract_image_block_output_dir\": image_output_dir,\n",
    "            }\n",
    "        ),\n",
    "        chunker_config=ChunkerConfig(\n",
    "            chunking_strategy=\"by_title\",\n",
    "            # Chunking params to aggregate text blocks\n",
    "            # Attempt to create a new chunk 3800 chars\n",
    "            # Attempt to keep chunks > 2000 chars\n",
    "            chunk_max_characters=4000,\n",
    "            chunk_new_after_n_chars=3800,\n",
    "            chunk_combine_text_under_n_chars=2000\n",
    "        ),\n",
    "        uploader_config=LocalUploaderConfig(\n",
    "            output_dir=output_dir,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a5346a9-f7c5-4b4b-a24c-7d77feb2a477",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 11:35:24,289 MainProcess INFO     Created index with configs: {\"input_path\": \"data/input-docs/LLaVA-subset.pdf\", \"recursive\": false, \"file_glob\": null}, connection configs: {\"access_config\": {}}\n",
      "2024-07-19 11:35:24,290 MainProcess INFO     Created download with configs: {\"download_dir\": null}, connection configs: {\"access_config\": {}}\n",
      "2024-07-19 11:35:24,292 MainProcess INFO     Created partition with configs: {\"strategy\": \"hi_res\", \"ocr_languages\": null, \"encoding\": null, \"additional_partition_args\": {\"languages\": [\"eng\"], \"extract_images_in_pdf\": true, \"extract_image_block_types\": [\"Image\", \"Table\"], \"extract_image_block_output_dir\": \"data/ingest-output/images\"}, \"skip_infer_table_types\": null, \"fields_include\": [\"element_id\", \"text\", \"type\", \"metadata\", \"embeddings\"], \"flatten_metadata\": false, \"metadata_exclude\": [], \"metadata_include\": [], \"partition_endpoint\": \"https://api.unstructured.io/general/v0/general\", \"partition_by_api\": false, \"api_key\": null, \"hi_res_model_name\": null}\n",
      "2024-07-19 11:35:24,294 MainProcess INFO     Created chunk with configs: {\"chunking_strategy\": \"by_title\", \"chunking_endpoint\": \"https://api.unstructured.io/general/v0/general\", \"chunk_by_api\": false, \"chunk_api_key\": null, \"chunk_combine_text_under_n_chars\": 2000, \"chunk_include_orig_elements\": null, \"chunk_max_characters\": 4000, \"chunk_multipage_sections\": null, \"chunk_new_after_n_chars\": 3800, \"chunk_overlap\": null, \"chunk_overlap_all\": null}\n",
      "2024-07-19 11:35:24,296 MainProcess INFO     Created upload with configs: {\"output_dir\": \"data/ingest-output/\"}, connection configs: {\"access_config\": {}}\n"
     ]
    }
   ],
   "source": [
    "ingest_pipeline = create_data_ingestion_pipeline(\n",
    "    input_pdf_path=\"data/input-docs/LLaVA-subset.pdf\",\n",
    "    output_dir=\"data/ingest-output/\",\n",
    "    image_output_dir=\"data/ingest-output/images\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76e77e48-ccb1-4183-9e72-89986a5d0a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index (LocalIndexer) -> download (LocalDownloader) -> partition (hi_res) -> chunk (by_title) -> upload (LocalUploader)\n"
     ]
    }
   ],
   "source": [
    "print(ingest_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac082c7d-dd67-41a9-8de5-b6cf6acb2b81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set to `True` if you want to re-run the partition and chunking step.\n",
    "# Leave it as `False` to re-use existing results.\n",
    "RUN_PARTITION_CHUNKING_STEP = False\n",
    "\n",
    "if RUN_PARTITION_CHUNKING_STEP:\n",
    "    ingest_pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1812aa1-ceb4-4dfa-b0c8-6bb9e63072ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.staging.base import elements_from_json\n",
    "\n",
    "elements = elements_from_json(\n",
    "    filename=\"data/ingest-output/LLaVA-subset.pdf.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "836151a6-94e2-4c50-b1e0-c2e7386c4728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({unstructured.documents.elements.CompositeElement: 17,\n",
       "         unstructured.documents.elements.Table: 4})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "display(\n",
    "    Counter(\n",
    "        type(element)        \n",
    "        for element \n",
    "        in elements\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5daf24d-f6b3-4f57-b25f-ed654d7c2477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.documents.elements import (\n",
    "    Element, \n",
    "    ElementType\n",
    ")\n",
    "\n",
    "def categorize_elements(\n",
    "    pdf_elements: list[Element]\n",
    ") -> tuple[list[str], list[str]]:\n",
    "    \"\"\"\n",
    "    Categorize partitioned elements from \n",
    "    a PDF document into tables and texts.\n",
    "    \n",
    "    Args:\n",
    "        pdf_elements: List of elements partitioned from PDF.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing the text elements' \n",
    "        contents and the table elements' contents.\n",
    "\n",
    "    \"\"\"\n",
    "    texts, tables = [], []\n",
    "    \n",
    "    for element in pdf_elements:\n",
    "        if element.category == ElementType.TABLE:\n",
    "            tables.append(element.text)\n",
    "        elif element.category == ElementType.COMPOSITE_ELEMENT:\n",
    "            texts.append(element.text)\n",
    "            \n",
    "    return texts, tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddab5083-d8b3-49e6-9b61-700868407ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, tables = categorize_elements(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f3049e6-27e3-405a-837e-1a0f1a39ab5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of table elements: 4\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Number of table elements: {len(tables)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79828833-f70b-4e25-a780-483736e85705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text elements: 17\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Number of text elements: {len(texts)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e04066-166c-4e2d-8db2-82564b022ee3",
   "metadata": {},
   "source": [
    "## Multi-vector retriever\n",
    "\n",
    "Use [multi-vector-retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector#summary) to index image (and / or text, table) summaries, but retrieve raw images (along with raw texts or tables).\n",
    "\n",
    "### Text and Table summaries\n",
    "\n",
    "We'll use **Gemini 1.5 Flash** to produce table and text summaries.\n",
    "\n",
    "Summaries are used to retrieve raw tables and / or raw chunks of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d66b7f08-9a06-4e85-aada-c749690f4295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "def generate_summaries(\n",
    "    texts_or_tables: list[str]\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Summarize given list of texts or tables.\n",
    "\n",
    "    Args:\n",
    "        texts_or_tables: List of texts or tables to summarize.\n",
    "\n",
    "    Returns:\n",
    "        List of summary for each piece of text or table.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt_text = (\n",
    "        \"You are an assistant tasked with summarizing tables \"\n",
    "        \"and text for retrieval. These summaries will be embedded \"\n",
    "        \"and used to retrieve the raw text or table elements. \"\n",
    "        \"Give a concise summary of the table or text that is \"\n",
    "        \"well optimized for retrieval.\\n\\n\"\n",
    "        \"Table or text:\\n\"\n",
    "        \"{element}\"\n",
    "    )\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        template=prompt_text\n",
    "    )\n",
    "\n",
    "    # Text summary chain\n",
    "    model = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash\", \n",
    "        temperature=0,\n",
    "        \n",
    "        # Set max_retries = 0 to avoid \n",
    "        # retrying on rate limit error\n",
    "        max_retries=0,\n",
    "    )\n",
    "    summarize_chain = (\n",
    "        {\"element\": lambda x: x} \n",
    "        | prompt \n",
    "        | model \n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # Initialize empty summaries\n",
    "    summaries = []\n",
    "\n",
    "    summaries = summarize_chain.batch(\n",
    "        inputs=texts_or_tables, \n",
    "        config={\n",
    "            \"max_concurrency\": 5\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e589a54d-4c77-4825-b466-c30f180939c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_SUMMARIZE_TEXTS_STEP = False\n",
    "\n",
    "# Gemini API Rate Limits for Free tier:\n",
    "# 15 RPM (requests per minute)\n",
    "if RUN_SUMMARIZE_TEXTS_STEP:\n",
    "    text_summaries = generate_summaries(\n",
    "        texts\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe8db7ca-2efb-4ddd-8240-7ff8d8bef4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_SUMMARIZE_TABLES_STEP = False\n",
    "\n",
    "if RUN_SUMMARIZE_TABLES_STEP:\n",
    "    table_summaries = generate_summaries(\n",
    "        tables    \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21409680-0b51-4d6b-9bad-070240391fb7",
   "metadata": {},
   "source": [
    "### Save summaries to JSON\n",
    "\n",
    "Save the texts, tables and images summaries to JSON files. Next time we can just load the summaries from the JSON files. Calling the LLM API to summarize each time is expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe13ff36-cb38-4d62-b0b3-48ddf5a934da",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SUMMARIZE_TEXTS_STEP:\n",
    "    from utils import write_to_json\n",
    "    \n",
    "    write_to_json(\n",
    "        summaries=text_summaries,\n",
    "        json_path='data/summaries/text_summaries.json'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c4e7ee2-03c7-432a-9c79-bd6ddcd516b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SUMMARIZE_TABLES_STEP:\n",
    "    from utils import write_to_json\n",
    "    \n",
    "    write_to_json(\n",
    "        summaries=table_summaries,\n",
    "        json_path='data/summaries/table_summaries.json'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6602c463-318a-48c5-9c3e-d40cc1410541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_from_json\n",
    "\n",
    "text_summaries = read_from_json('data/summaries/text_summaries.json')\n",
    "table_summaries = read_from_json('data/summaries/table_summaries.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5593b243-0b6c-4da3-ac21-a51f3f157c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       ">LLaVA is a large multimodal model that combines a vision encoder and an LLM for visual and language understanding. It was trained using language-only GPT-4 generated multimodal language-image instruction-following data. LLaVA demonstrates impressive multimodal chat abilities and achieves a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, LLaVA achieves a new state-of-the-art accuracy of 92.53%. The model, code, and GPT-4 generated visual instruction tuning data are publicly available. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(\">\" + text_summaries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "584add35-0a2b-4b84-8dfc-63537d7e7559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       ">This table shows the performance of a model on a conversation task, with different levels of detail and complex reasoning. The model was trained with and without instruction tuning. The results show that the model performs best with full data and instruction tuning, and that performance decreases as the amount of detail and complex reasoning increases. The model also performs worse without instruction tuning. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(\">\" + table_summaries[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51027470-d778-47d0-83c8-ff14e5aa7bd8",
   "metadata": {},
   "source": [
    "### Image summaries \n",
    "\n",
    "We will use Gemini 1.5 Flash to produce the image summaries.\n",
    "\n",
    "The API docs [here](https://ai.google.dev/gemini-api/docs/vision?lang=python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f89c8e00-1fec-4eac-b186-29f1687d2ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def summarize_image(\n",
    "    image_base64: str, \n",
    "    prompt: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a summary for a given image.\n",
    "\n",
    "    Args:\n",
    "        image_base64: Image encoded as a Base64 string.\n",
    "        prompt: The prompt given to the LLM to summarize the image.\n",
    "\n",
    "    Returns:\n",
    "        Summary or brief description of the image.\n",
    "        \n",
    "    \"\"\"    \n",
    "    model = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash\", \n",
    "        max_tokens=1024        \n",
    "    )\n",
    "\n",
    "    message = model.invoke(\n",
    "        input=[\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\n",
    "                        \"type\": \"text\", \n",
    "                        \"text\": prompt\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{image_base64}\"\n",
    "                        },\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "381847ef-1461-4587-8790-e01f439a55fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "from pathlib import Path\n",
    "from utils import encode_image\n",
    "\n",
    "\n",
    "def generate_image_summaries(\n",
    "    image_paths: Iterable[Path]\n",
    ") -> tuple[list[str], list[str]]:\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "\n",
    "    Args:\n",
    "        image_paths: Paths to the images.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing a list of images encoded as Base64 string,\n",
    "        and a list of summaries for the respective images.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Store base64 encoded images\n",
    "    image_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "    \n",
    "    prompt = (\n",
    "        \"You are an assistant tasked with summarizing images \"\n",
    "        \"for retrieval. These summaries will be embedded and \"\n",
    "        \"used to retrieve the raw image. Give a concise summary \"\n",
    "        \"of the image that is well optimized for retrieval.\"\n",
    "    )\n",
    "\n",
    "    for path in image_paths:\n",
    "        base64_image = encode_image(path)\n",
    "        image_base64_list.append(base64_image)\n",
    "\n",
    "        # Generate a summary for an image and \n",
    "        # add it to the list.\n",
    "        image_summaries.append(\n",
    "            summarize_image(\n",
    "                base64_image, \n",
    "                prompt\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return image_base64_list, image_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66be1a71-4ea3-41b5-a56a-90042be15c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/ingest-output/images/figure-3-1.jpg'),\n",
       " PosixPath('data/ingest-output/images/figure-4-2.jpg'),\n",
       " PosixPath('data/ingest-output/images/figure-6-3.jpg'),\n",
       " PosixPath('data/ingest-output/images/figure-8-4.jpg'),\n",
       " PosixPath('data/ingest-output/images/figure-8-5.jpg')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "image_dir_path = Path(\"data/ingest-output/images\")\n",
    "sorted_image_paths = sorted(\n",
    "    # Only summarize images (figures) and not images of tables.\n",
    "    # The tables have already been summarized previously.\n",
    "    image_dir_path.glob(\"figure-*.jpg\")\n",
    ")\n",
    "sorted_image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "18130c07-1480-4e26-889e-19453edf89ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_SUMMARIZE_IMAGES_STEP = False\n",
    "\n",
    "if RUN_SUMMARIZE_IMAGES_STEP:\n",
    "    image_base64_list, image_summaries = generate_image_summaries(\n",
    "        image_paths=sorted_image_paths\n",
    "    )\n",
    "\n",
    "    # Save the results to JSON files, so that we \n",
    "    # can just load it in the next time.\n",
    "    from utils import write_to_json\n",
    "    \n",
    "    write_to_json(\n",
    "        summaries=image_summaries,\n",
    "        json_path='data/summaries/image_summaries.json'\n",
    "    )\n",
    "    write_to_json(\n",
    "        summaries=image_base64_list,\n",
    "        json_path='data/summaries/images_base64.json'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1714ff37-f37e-476f-aae2-beac2051c7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_from_json\n",
    "\n",
    "image_summaries = read_from_json(\n",
    "    json_path='data/summaries/image_summaries.json'\n",
    ")\n",
    "image_base64_list = read_from_json(\n",
    "    json_path='data/summaries/images_base64.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc1b6632-90a2-4d0c-afd7-7bb5c349a7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/jpeg;base64,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\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "A black SUV is parked in a garage with a person loading luggage into the back.  There are multiple bags and suitcases in the garage.  A second person is standing next to the car.  The garage is empty and has concrete walls.  There are multiple fire extinguishers on the wall."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import plt_img_base64\n",
    "\n",
    "plt_img_base64(image_base64_list[0])\n",
    "Markdown(image_summaries[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705d7ce4-f0ef-4a4a-816e-0392c6da4514",
   "metadata": {},
   "source": [
    "### Add to vectorstore\n",
    "\n",
    "Add raw docs and doc summaries to [Multi Vector Retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector): \n",
    "\n",
    "* Store the raw texts, tables, and images in the `docstore`.\n",
    "* Store the texts, table summaries, and image summaries in the `vectorstore` for efficient semantic retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "07ec8a0e-a4b2-4302-9d68-fed8d37f44cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "\n",
    "def generate_random_ids(\n",
    "    count: int\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Generates a list of random UUID.\n",
    "\n",
    "    Args:\n",
    "        count: Number of random UUID to generate.\n",
    "\n",
    "    Returns:\n",
    "        A list of random UUID with length of `count`.\n",
    "        \n",
    "    \"\"\"\n",
    "    return [\n",
    "        str(uuid.uuid4()) \n",
    "        for _ in range(count)\n",
    "    ]\n",
    "\n",
    "\n",
    "def create_documents_from_texts(\n",
    "    id_key: str,\n",
    "    texts: list[str],\n",
    "    doc_ids: list[str],\n",
    ") -> list[Document]:\n",
    "    \"\"\"\n",
    "    Creates a LangChain `Document` object for each text content.\n",
    "\n",
    "    Args:\n",
    "        id_key: Key that is paired with the document ID value.\n",
    "        texts: List of text contents.\n",
    "        doc_ids: List of unique IDs for each document.\n",
    "\n",
    "    Returns:\n",
    "        List of LangChain `Document` objects.\n",
    "    \n",
    "    \"\"\"\n",
    "    return [\n",
    "        Document(\n",
    "            page_content=text, \n",
    "            metadata={\n",
    "                id_key: doc_ids[index]\n",
    "            }\n",
    "        )\n",
    "        for index, text in enumerate(texts)\n",
    "    ]\n",
    "\n",
    "\n",
    "def add_documents(    \n",
    "    retriever: MultiVectorRetriever, \n",
    "    summaries: list[str], \n",
    "    contents: list[str],\n",
    "):\n",
    "    \"\"\"\n",
    "    Add summaries to the vector store and add contents to the\n",
    "    docstore of the Multi Vector Retriever.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Generate a unique ID for each Document.\n",
    "    doc_ids = generate_random_ids(\n",
    "        count=len(contents)\n",
    "    )\n",
    "\n",
    "    # Create a LangChain Document for each summary.\n",
    "    summary_docs = create_documents_from_texts(\n",
    "        id_key=retriever.id_key,\n",
    "        texts=summaries,\n",
    "        doc_ids=doc_ids\n",
    "    )\n",
    "    \n",
    "    retriever.vectorstore.add_documents(\n",
    "        documents=summary_docs\n",
    "    )\n",
    "    retriever.docstore.mset(\n",
    "        key_value_pairs=list(\n",
    "            zip(doc_ids, contents)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ecb9e95f-1961-4bc3-8cb0-f06a7825738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "\n",
    "def create_multi_vector_retriever(\n",
    "    vector_store, \n",
    "    text_summaries, \n",
    "    texts, \n",
    "    table_summaries, \n",
    "    tables, \n",
    "    image_summaries, \n",
    "    images\n",
    "):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, \n",
    "    but returns raw images or texts.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initialize the storage layer\n",
    "    mem_store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "    \n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vector_store,\n",
    "        docstore=mem_store,\n",
    "        id_key=id_key,\n",
    "    )    \n",
    "\n",
    "    # Add texts, tables, and images.\n",
    "    # Check that summaries is not empty before adding.    \n",
    "    if text_summaries:\n",
    "        add_documents(            \n",
    "            retriever, \n",
    "            text_summaries, \n",
    "            texts\n",
    "        )  \n",
    "        \n",
    "    if table_summaries:\n",
    "        add_documents(            \n",
    "            retriever, \n",
    "            table_summaries, \n",
    "            tables\n",
    "        )\n",
    "        \n",
    "    if image_summaries:\n",
    "        add_documents(            \n",
    "            retriever, \n",
    "            image_summaries, \n",
    "            images\n",
    "        )\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "262b7e22-6489-4995-91d5-917ec532aa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "\n",
    "vector_store = Chroma(    \n",
    "    collection_name=\"multi_modal_rag\", \n",
    "    embedding_function=GoogleGenerativeAIEmbeddings(\n",
    "        model=\"models/text-embedding-004\"\n",
    "    ),\n",
    "    persist_directory='data/vector-store',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ebde6495-578c-4edc-997d-40e5935bf6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever\n",
    "retriever_multi_vector = create_multi_vector_retriever(\n",
    "    vector_store,\n",
    "    text_summaries,\n",
    "    texts,\n",
    "    table_summaries,\n",
    "    tables,\n",
    "    image_summaries,\n",
    "    image_base64_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "12789cd7-9441-49c3-aadb-9a2ae800af98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'doc_id': 'ff8bb037-91a9-4338-9d97-fe6ca333376c'}, page_content=\"This paper introduces visual instruction-tuning, a novel approach to building a general-purpose visual assistant. It addresses the lack of vision-language instruction-following data by converting image-text pairs into an instruction-following format using ChatGPT/GPT-4. The paper also presents a large multimodal model (LMM) by connecting CLIP's visual encoder with Vicuna's language decoder and fine-tuning it on the generated data. The LMM achieves state-of-the-art performance on the Science QA dataset when combined with GPT-4. Additionally, the paper introduces LLaVA-Bench, a multimodal instruction-following benchmark with challenging tasks, and releases the generated data, codebase, model checkpoints, and a visual chat demo. \\n\")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_docs = vector_store.similarity_search(\n",
    "    query=\"Visual Intruction Tuning\",\n",
    "    k=1\n",
    ")\n",
    "summary_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "49b77de0-2e48-4625-8a5a-de7b01746406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check retrieval\n",
    "docs = retriever_multi_vector.invoke(\n",
    "    input=\"ramen\",     \n",
    ")\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182edbb3-16a3-4f95-9b0f-5f4f785e46fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1215c3-ad5c-4428-afd8-c131925dac3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
