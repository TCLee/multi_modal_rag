{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d63639f-7af0-4971-9542-2cd76910dd89",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before we begin, please make sure you have setup the `.env` file in the project \n",
    "directory as described in [`README.md`](README.md).\n",
    "\n",
    "Next, we will load in the necessary environment variables (e.g., API keys) for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ca59d65-2e5a-496a-9992-4fe09ecb6922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()\n",
    "\n",
    "assert os.environ.get(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afbb58b-ad5d-4d41-b739-cd4bc5e47bce",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "### Partioning and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74157174-79c9-4a0d-826a-d4740f0f995e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-15 18:09:44,323 MainProcess INFO     Created index with configs: {\"input_path\": \"data/input-docs/LLaVA-subset.pdf\", \"recursive\": false, \"file_glob\": null}, connection configs: {\"access_config\": {}}\n",
      "2024-07-15 18:09:44,324 MainProcess INFO     Created download with configs: {\"download_dir\": null}, connection configs: {\"access_config\": {}}\n",
      "2024-07-15 18:09:44,325 MainProcess INFO     Created partition with configs: {\"strategy\": \"hi_res\", \"ocr_languages\": null, \"encoding\": null, \"additional_partition_args\": {\"languages\": [\"eng\"], \"extract_images_in_pdf\": true, \"extract_image_block_types\": [\"Image\", \"Table\"], \"extract_image_block_output_dir\": \"data/ingest-output/images\"}, \"skip_infer_table_types\": null, \"fields_include\": [\"element_id\", \"text\", \"type\", \"metadata\", \"embeddings\"], \"flatten_metadata\": false, \"metadata_exclude\": [], \"metadata_include\": [], \"partition_endpoint\": \"https://api.unstructured.io/general/v0/general\", \"partition_by_api\": false, \"api_key\": null, \"hi_res_model_name\": null}\n",
      "2024-07-15 18:09:44,327 MainProcess INFO     Created chunk with configs: {\"chunking_strategy\": \"by_title\", \"chunking_endpoint\": \"https://api.unstructured.io/general/v0/general\", \"chunk_by_api\": false, \"chunk_api_key\": null, \"chunk_combine_text_under_n_chars\": 2000, \"chunk_include_orig_elements\": null, \"chunk_max_characters\": 4000, \"chunk_multipage_sections\": null, \"chunk_new_after_n_chars\": 3800, \"chunk_overlap\": null, \"chunk_overlap_all\": null}\n",
      "2024-07-15 18:09:44,328 MainProcess INFO     Created upload with configs: {\"output_dir\": \"data/ingest-output/\"}, connection configs: {\"access_config\": {}}\n"
     ]
    }
   ],
   "source": [
    "from unstructured.ingest.v2.pipeline.pipeline import Pipeline\n",
    "from unstructured.ingest.v2.interfaces import ProcessorConfig\n",
    "from unstructured.ingest.v2.processes.connectors.local import (\n",
    "    LocalIndexerConfig,\n",
    "    LocalDownloaderConfig,\n",
    "    LocalConnectionConfig,\n",
    "    LocalUploaderConfig\n",
    ")\n",
    "from unstructured.ingest.v2.processes.partitioner import PartitionerConfig\n",
    "from unstructured.ingest.v2.processes.chunker import ChunkerConfig\n",
    "\n",
    "\n",
    "ingest_pipeline = Pipeline.from_configs(\n",
    "    context=ProcessorConfig(\n",
    "        verbose=True,\n",
    "        tqdm=True,\n",
    "    ),\n",
    "    indexer_config=LocalIndexerConfig(\n",
    "        input_path=\"data/input-docs/LLaVA-subset.pdf\"\n",
    "    ),\n",
    "    downloader_config=LocalDownloaderConfig(),\n",
    "    source_connection_config=LocalConnectionConfig(),\n",
    "    partitioner_config=PartitionerConfig(\n",
    "        partition_by_api=False,\n",
    "        strategy=\"hi_res\",        \n",
    "        additional_partition_args={\n",
    "            \"languages\": [\"eng\"],            \n",
    "            \"extract_images_in_pdf\": True,\n",
    "            \"extract_image_block_types\": [\"Image\", \"Table\"],\n",
    "            \"extract_image_block_output_dir\": \"data/ingest-output/images\",\n",
    "        }\n",
    "    ),\n",
    "    chunker_config=ChunkerConfig(\n",
    "        chunking_strategy=\"by_title\",\n",
    "        # Chunking params to aggregate text blocks\n",
    "        # Attempt to create a new chunk 3800 chars\n",
    "        # Attempt to keep chunks > 2000 chars\n",
    "        chunk_max_characters=4000,\n",
    "        chunk_new_after_n_chars=3800,\n",
    "        chunk_combine_text_under_n_chars=2000\n",
    "    ),\n",
    "    uploader_config=LocalUploaderConfig(\n",
    "        output_dir=\"data/ingest-output/\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e77e48-ccb1-4183-9e72-89986a5d0a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index (LocalIndexer) -> download (LocalDownloader) -> partition (hi_res) -> chunk (by_title) -> upload (LocalUploader)\n"
     ]
    }
   ],
   "source": [
    "print(ingest_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac082c7d-dd67-41a9-8de5-b6cf6acb2b81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-15 18:09:47,048 MainProcess INFO     Running local pipline: index (LocalIndexer) -> download (LocalDownloader) -> partition (hi_res) -> chunk (by_title) -> upload (LocalUploader) with configs: {\"reprocess\": false, \"verbose\": true, \"tqdm\": true, \"work_dir\": \"/Users/tclee/.cache/unstructured/ingest/pipeline\", \"num_processes\": 2, \"max_connections\": null, \"raise_on_error\": false, \"disable_parallelism\": false, \"preserve_downloads\": false, \"download_only\": false, \"max_docs\": null, \"re_download\": false, \"uncompress\": false, \"status\": {}, \"semaphore\": null}\n",
      "2024-07-15 18:09:47,153 MainProcess DEBUG    Generated file data: FileData(identifier='/Users/tclee/notebooks/multi_modal_rag/data/input-docs/LLaVA-subset.pdf', connector_type='local', source_identifiers=SourceIdentifiers(filename='LLaVA-subset.pdf', fullpath='/Users/tclee/notebooks/multi_modal_rag/data/input-docs/LLaVA-subset.pdf', rel_path='LLaVA-subset.pdf'), doc_type=<IndexDocType.FILE: 'file'>, metadata=DataSourceMetadata(url=None, version=None, record_locator={\"path\": \"/Users/tclee/notebooks/multi_modal_rag/data/input-docs/LLaVA-subset.pdf\"}, date_created='1720856436.7616913', date_modified='1720856436.763505', date_processed='1721038187.153353', permissions_data=[{\"mode\": 33188}]), additional_metadata={}, reprocess=False)\n",
      "2024-07-15 18:09:47,156 MainProcess INFO     Calling DownloadStep with 1 docs\n",
      "2024-07-15 18:09:47,156 MainProcess INFO     processing content async\n",
      "2024-07-15 18:09:47,156 MainProcess WARNING  async code being run in dedicated thread pool to not conflict with existing event loop: <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "2024-07-15 18:09:47,162 MainProcess DEBUG    Skipping download, file already exists locally: /Users/tclee/notebooks/multi_modal_rag/data/input-docs/LLaVA-subset.pdf\n",
      "2024-07-15 18:09:47,164 MainProcess INFO     DownloadStep [cls] took 0.008231878280639648 seconds\n",
      "2024-07-15 18:09:47,164 MainProcess INFO     Calling PartitionStep with 1 docs\n",
      "2024-07-15 18:09:47,165 MainProcess INFO     processing content across processes\n",
      "2024-07-15 18:09:47,166 MainProcess INFO     processing content serially\n",
      "2024-07-15 18:09:47,166 MainProcess WARNING  async code being run in dedicated thread pool to not conflict with existing event loop: <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "2024-07-15 18:09:47,169 MainProcess DEBUG    Using local partition with kwargs: {\"strategy\": \"hi_res\", \"languages\": [\"eng\"], \"extract_images_in_pdf\": true, \"extract_image_block_types\": [\"Image\", \"Table\"], \"extract_image_block_output_dir\": \"data/ingest-output/images\"}\n",
      "2024-07-15 18:09:47,171 MainProcess DEBUG    partitioning file /Users/tclee/notebooks/multi_modal_rag/data/input-docs/LLaVA-subset.pdf with metadata {\"record_locator\": {\"path\": \"/Users/tclee/notebooks/multi_modal_rag/data/input-docs/LLaVA-subset.pdf\"}, \"date_created\": \"1720856436.7616913\", \"date_modified\": \"1720856436.763505\", \"date_processed\": \"1721038187.153353\", \"permissions_data\": [{\"mode\": 33188}]}\n",
      "2024-07-15 18:10:59,570 MainProcess DEBUG    Writing partitioned output to: /Users/tclee/.cache/unstructured/ingest/pipeline/partition/262e9237d0a0.json\n",
      "2024-07-15 18:10:59,580 MainProcess INFO     PartitionStep [cls] took 72.41550207138062 seconds\n",
      "2024-07-15 18:10:59,580 MainProcess INFO     Calling ChunkStep with 1 docs\n",
      "2024-07-15 18:10:59,580 MainProcess INFO     processing content across processes\n",
      "2024-07-15 18:10:59,581 MainProcess INFO     processing content serially\n",
      "2024-07-15 18:10:59,581 MainProcess WARNING  async code being run in dedicated thread pool to not conflict with existing event loop: <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "2024-07-15 18:10:59,640 MainProcess DEBUG    Writing chunker output to: /Users/tclee/.cache/unstructured/ingest/pipeline/chunk/579cffa85aed.json\n",
      "2024-07-15 18:10:59,643 MainProcess INFO     ChunkStep [cls] took 0.06249213218688965 seconds\n",
      "2024-07-15 18:10:59,643 MainProcess INFO     Calling UploadStep with 1 docs\n",
      "2024-07-15 18:10:59,645 MainProcess DEBUG    copying file from /Users/tclee/.cache/unstructured/ingest/pipeline/chunk/579cffa85aed.json to /Users/tclee/notebooks/multi_modal_rag/data/ingest-output/LLaVA-subset.pdf.json\n",
      "2024-07-15 18:10:59,648 MainProcess INFO     UploadStep [cls] took 0.004611015319824219 seconds\n",
      "2024-07-15 18:10:59,649 MainProcess INFO     Finished ingest process in 72.60117888450623s\n"
     ]
    }
   ],
   "source": [
    "ingest_pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1812aa1-ceb4-4dfa-b0c8-6bb9e63072ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.staging.base import elements_from_json\n",
    "\n",
    "elements = elements_from_json(\n",
    "    filename=\"data/ingest-output/LLaVA-subset.pdf.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "836151a6-94e2-4c50-b1e0-c2e7386c4728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({unstructured.documents.elements.CompositeElement: 17,\n",
       "         unstructured.documents.elements.Table: 4})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "display(\n",
    "    Counter(\n",
    "        type(element) \n",
    "        for element \n",
    "        in elements\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98e9a68a-a43b-4ae9-a8a7-56dc3b9086f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize elements by type\n",
    "def categorize_elements(raw_pdf_elements):\n",
    "    \"\"\"\n",
    "    Categorize extracted elements from a PDF into tables and texts.\n",
    "    \n",
    "    raw_pdf_elements: List of unstructured.documents.elements\n",
    "    \"\"\"\n",
    "    tables, texts = ([], [])\n",
    "    \n",
    "    for element in raw_pdf_elements:\n",
    "        if element.category == \"Table\":\n",
    "            tables.append(element.text)\n",
    "        elif element.category == \"CompositeElement\":\n",
    "            texts.append(element.text)\n",
    "            \n",
    "    return texts, tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddab5083-d8b3-49e6-9b61-700868407ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get text, tables\n",
    "texts, tables = categorize_elements(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f3049e6-27e3-405a-837e-1a0f1a39ab5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79828833-f70b-4e25-a780-483736e85705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e04066-166c-4e2d-8db2-82564b022ee3",
   "metadata": {},
   "source": [
    "## Multi-vector retriever\n",
    "\n",
    "Use [multi-vector-retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector#summary) to index image (and / or text, table) summaries, but retrieve raw images (along with raw texts or tables).\n",
    "\n",
    "### Text and Table summaries\n",
    "\n",
    "We'll use **Gemini 1.5 Flash** to produce table and text summaries.\n",
    "\n",
    "Summaries are used to retrieve raw tables and / or raw chunks of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b50c6bb0-fd9d-4c59-a3f7-f249c5e147c0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "# Generate summaries of text and table elements\n",
    "def generate_summaries(texts, tables):\n",
    "    \"\"\"\n",
    "    Summarize text elements\n",
    "    texts: List of str\n",
    "    tables: List of str    \n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt_text = (\n",
    "        \"You are an assistant tasked with summarizing tables \"\n",
    "        \"and text for retrieval. These summaries will be embedded \"\n",
    "        \"and used to retrieve the raw text or table elements. \"\n",
    "        \"Give a concise summary of the table or text that is \"\n",
    "        \"well optimized for retrieval.\\n\\n\"\n",
    "        \"Table or text:\\n\"\n",
    "        \"{element}\"\n",
    "    )\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        prompt_text\n",
    "    )\n",
    "\n",
    "    # Text summary chain\n",
    "    model = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash\", \n",
    "        temperature=0\n",
    "    )    \n",
    "    summarize_chain = (\n",
    "        {\"element\": lambda x: x} \n",
    "        | prompt \n",
    "        | model \n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # Initialize empty summaries\n",
    "    text_summaries, table_summaries = ([], [])\n",
    "\n",
    "    text_summaries = summarize_chain.batch(\n",
    "        inputs=texts, \n",
    "        config={\n",
    "            \"max_concurrency\": 5\n",
    "        }\n",
    "    )\n",
    "\n",
    "    table_summaries = summarize_chain.batch(\n",
    "        inputs=tables, \n",
    "        config={\n",
    "            \"max_concurrency\": 5\n",
    "        }\n",
    "    )    \n",
    "\n",
    "    return (text_summaries, table_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d66b7f08-9a06-4e85-aada-c749690f4295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "# Generate summaries of text or table elements\n",
    "def generate_summaries(\n",
    "    texts: list[str]\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Summarize text elements\n",
    "    texts: List of str\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt_text = (\n",
    "        \"You are an assistant tasked with summarizing tables \"\n",
    "        \"and text for retrieval. These summaries will be embedded \"\n",
    "        \"and used to retrieve the raw text or table elements. \"\n",
    "        \"Give a concise summary of the table or text that is \"\n",
    "        \"well optimized for retrieval.\\n\\n\"\n",
    "        \"Table or text:\\n\"\n",
    "        \"{element}\"\n",
    "    )\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        template=prompt_text\n",
    "    )\n",
    "\n",
    "    # Text summary chain\n",
    "    model = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash\", \n",
    "        temperature=0\n",
    "    )    \n",
    "    summarize_chain = (\n",
    "        {\"element\": lambda x: x} \n",
    "        | prompt \n",
    "        | model \n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # Initialize empty summaries\n",
    "    summaries = []\n",
    "\n",
    "    summaries = summarize_chain.batch(\n",
    "        inputs=texts, \n",
    "        config={\n",
    "            \"max_concurrency\": 5\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e589a54d-4c77-4825-b466-c30f180939c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini API Rate Limits for Free tier:\n",
    "# 15 RPM (requests per minute)\n",
    "text_summaries_1 = generate_summaries(\n",
    "    texts[:15]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0266817-80fa-4bb7-bbe5-060e2525dedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summaries_2 = generate_summaries(\n",
    "    texts[15:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe8db7ca-2efb-4ddd-8240-7ff8d8bef4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_summaries = generate_summaries(\n",
    "    tables    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "720f919f-ba2c-41b3-9e3e-e82234ae3910",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summaries = text_summaries_1 + text_summaries_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21409680-0b51-4d6b-9bad-070240391fb7",
   "metadata": {},
   "source": [
    "### Save summaries to JSON\n",
    "\n",
    "Save the texts, tables and images summaries to JSON files. Next time we can just load the summaries from the JSON files. Calling the LLM API to summarize each time is expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8139608e-0ad5-41ff-a8a4-ae2f87c7f943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_to_json(summaries: list[str], json_path: str):\n",
    "    \"\"\"\n",
    "    Saves list of summaries to a JSON file.\n",
    "    \"\"\"\n",
    "    with open(\n",
    "        file=json_path, \n",
    "        mode='w', \n",
    "        encoding='utf-8') as f:\n",
    "        json.dump(\n",
    "            summaries, \n",
    "            f,        \n",
    "            ensure_ascii=False,             \n",
    "            indent=4\n",
    "        )\n",
    "\n",
    "\n",
    "def read_from_json(json_path: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of summaries from a JSON file.\n",
    "    \"\"\"\n",
    "    with open(\n",
    "        file=json_path, \n",
    "        mode='r', \n",
    "        encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fe13ff36-cb38-4d62-b0b3-48ddf5a934da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_to_json(\n",
    "#     summaries=text_summaries,\n",
    "#     json_path='data/summaries/text_summaries.json'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c4e7ee2-03c7-432a-9c79-bd6ddcd516b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_to_json(\n",
    "#     summaries=table_summaries,\n",
    "#     json_path='data/summaries/table_summaries.json'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6602c463-318a-48c5-9c3e-d40cc1410541",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summaries = read_from_json('data/summaries/text_summaries.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "12a96727-954f-4883-80b3-4a19fb2c9b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_summaries = read_from_json('data/summaries/table_summaries.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4d26e9-7478-4369-83c2-d73449ca8d06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a51c6d-bf64-4801-b489-c58cc32a03b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
